{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://people.idsia.ch/~rupesh/very_deep_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "# wget https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{y} = H(\\textbf{x}, \\textbf{W}_\\textbf{H}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dense_layer(x, input_size, output_size, activation):\n",
    "    W = tf.Variable(tf.truncated_normal([input_size, output_size], stddev=0.1), name='weight')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[output_size]), name='bias')\n",
    "    y = activation(tf.matmul(x, W) + b)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\textbf{y} &= H(\\textbf{x}, \\textbf{W}_\\textbf{H}) \\cdot T(\\textbf{x}, \\textbf{W}_\\textbf{T}) + \\textbf{x} \\cdot C(\\textbf{x}, \\textbf{W}_\\textbf{C}) \\\\\n",
    "  \\textbf{y} &= H(\\textbf{x}, \\textbf{W}_\\textbf{H}) \\cdot T(\\textbf{x}, \\textbf{W}_\\textbf{T}) + \\textbf{x} \\cdot (1 - T(x, \\textbf{W}_\\textbf{T}))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway_layer(x, size, activation, carry_bias=-1.0):\n",
    "    W = tf.Variable(tf.truncated_normal([size, size], stddev=0.1), name='weight')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[size]), name='bias')\n",
    "\n",
    "    W_T = tf.Variable(tf.truncated_normal([size, size], stddev=0.1), name='weight_transform')\n",
    "    b_T = tf.Variable(tf.constant(carry_bias, shape=[size]), name='bias_transform')\n",
    "\n",
    "    H = activation(tf.matmul(x, W) + b, name='activation')\n",
    "    T = tf.sigmoid(tf.matmul(x, W_T) + b_T, name='transform_gate')\n",
    "    C = tf.sub(1.0, T, name=\"carry_gate\")\n",
    "\n",
    "    y = tf.add(tf.mul(H, T), tf.mul(x, C), 'y') # y = (H * T) + (x * C)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/andy/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/andy/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/andy/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/andy/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(os.path.expanduser('~') + \"/data/mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_layer_size = 784\n",
    "hidden_layer_size = 50 # use ~71 for fully-connected (plain) layers, 50 for highway layers\n",
    "output_layer_size = 10\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, input_layer_size])\n",
    "y_ = tf.placeholder(\"float\", [None, output_layer_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_count = 50\n",
    "carry_bias_init = -2.0\n",
    "\n",
    "prev_y = None\n",
    "y = None\n",
    "for i in range(layer_count):\n",
    "    with tf.name_scope(\"layer{0}\".format(i)) as scope:\n",
    "        if i == 0: # first, input layer\n",
    "            prev_y = dense_layer(x, input_layer_size, hidden_layer_size, tf.nn.relu)\n",
    "        elif i == layer_count - 1: # last, output layer\n",
    "            y = dense_layer(prev_y, hidden_layer_size, output_layer_size, tf.nn.softmax)\n",
    "        else: # hidden layers\n",
    "            # prev_y = dense_layer(prev_y, hidden_layer_size, hidden_layer_size, tf.nn.relu)\n",
    "            prev_y = highway_layer(prev_y, hidden_layer_size, tf.nn.relu, carry_bias=carry_bias_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "    cross_entropy_summary = tf.scalar_summary(\"loss\", cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"test\") as scope:\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_logs_1451456027/\n"
     ]
    }
   ],
   "source": [
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "log_path = \"mnist_logs_{0}/\".format(int(time.time()))\n",
    "print(log_path)\n",
    "writer = tf.train.SummaryWriter(log_path, sess.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.initialize_all_variables().run()\n",
    "mini_batch_size = 50\n",
    "num_epochs = 20\n",
    "iter_per_epoch = int(mnist.train.num_examples / mini_batch_size)\n",
    "num_iter = num_epochs * iter_per_epoch\n",
    "print(\"Running %d epochs with mini batch size of %d\" % (num_epochs, mini_batch_size))\n",
    "print(\"With %d iterations per epoch for a total of %d iterations\" % (iter_per_epoch, num_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, epoch 0, training accuracy 0.1\n",
      "test accuracy 0.101\n",
      "iter 100, epoch 0, training accuracy 0.12\n",
      "test accuracy 0.098\n",
      "iter 200, epoch 0, training accuracy 0.16\n",
      "test accuracy 0.1135\n",
      "iter 300, epoch 0, training accuracy 0.06\n",
      "test accuracy 0.098\n",
      "iter 400, epoch 0, training accuracy 0.12\n",
      "test accuracy 0.1135\n",
      "iter 500, epoch 0, training accuracy 0.08\n",
      "test accuracy 0.1009\n",
      "iter 600, epoch 0, training accuracy 0.02\n",
      "test accuracy 0.1028\n",
      "iter 700, epoch 0, training accuracy 0.12\n",
      "test accuracy 0.1135\n",
      "iter 800, epoch 0, training accuracy 0.1\n",
      "test accuracy 0.1028\n",
      "iter 900, epoch 0, training accuracy 0.14\n",
      "test accuracy 0.101\n",
      "iter 1000, epoch 0, training accuracy 0.1\n",
      "test accuracy 0.101\n",
      "iter 1100, epoch 1, training accuracy 0.2\n",
      "test accuracy 0.1028\n",
      "iter 1200, epoch 1, training accuracy 0.06\n",
      "test accuracy 0.098\n",
      "iter 1300, epoch 1, training accuracy 0.06\n",
      "test accuracy 0.1135\n",
      "iter 1400, epoch 1, training accuracy 0.32\n",
      "test accuracy 0.2545\n",
      "iter 1500, epoch 1, training accuracy 0.36\n",
      "test accuracy 0.4224\n",
      "iter 1600, epoch 1, training accuracy 0.72\n",
      "test accuracy 0.6617\n",
      "iter 1700, epoch 1, training accuracy 0.74\n",
      "test accuracy 0.6958\n",
      "iter 1800, epoch 1, training accuracy 0.7\n",
      "test accuracy 0.7465\n",
      "iter 1900, epoch 1, training accuracy 0.8\n",
      "test accuracy 0.8112\n",
      "iter 2000, epoch 1, training accuracy 0.76\n",
      "test accuracy 0.7577\n",
      "iter 2100, epoch 1, training accuracy 0.86\n",
      "test accuracy 0.8839\n",
      "iter 2200, epoch 2, training accuracy 0.9\n",
      "test accuracy 0.8987"
     ]
    }
   ],
   "source": [
    "for i in range(num_iter):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(50)\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    summary_str, train_accuracy = sess.run([merged, accuracy], feed_dict={\n",
    "        x: batch_xs,\n",
    "        y_: batch_ys,\n",
    "    })\n",
    "    writer.add_summary(summary_str, i)\n",
    "    print(\"iter %d, epoch %d, training accuracy %g\" % (i, i / iter_per_epoch, train_accuracy))\n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={ x: mnist.test.images, y_: mnist.test.labels }))\n",
    "\n",
    "  train_step.run(feed_dict={ x: batch_xs, y_: batch_ys })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
